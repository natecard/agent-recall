{
	"project": "Agent Recall Super Ralph",
	"version": 2,
	"selection_policy": {
		"agent_decides_priority": true,
		"priority_scale": "1 = highest priority",
		"order_of_operations": ["critical bugfixes", "tracer-bullet feature slices", "polish and quick wins", "refactors"]
	},
	"items": [
		{
			"id": "AR-701",
			"priority": 1,
			"title": "Claude Code Hook Integration",
			"description": "Inject PreToolUse and PostToolUse hooks into .claude/settings.json that call back to Ralph. PreToolUse hook blocks dangerous operations (rm -rf, DROP TABLE, etc) via a guardrails script generated from GUARDRAILS.md. PostToolUse hook logs tool-call events to Ralph's iteration store for Weather Model consumption.",
			"user_story": "As a developer using Ralph with Claude Code, I want Ralph to enforce my guardrails in real-time so that the AI agent cannot accidentally delete critical files or run destructive commands.",
			"steps": [
				"Create hook script generator that reads GUARDRAILS.md and produces a PreToolUse validation script",
				"Create PostToolUse logging script that appends structured events to .agent/ralph/tool_events.jsonl",
				"Add `ralph hooks install` CLI command to write hook configs into .claude/settings.json",
				"Add `ralph hooks uninstall` command to clean up",
				"Wire hook installation into Ralph loop startup",
				"Add TUI palette action for hook management"
			],
			"acceptance_criteria": [
				"Running `ralph hooks install` adds PreToolUse and PostToolUse entries to .claude/settings.json",
				"PreToolUse hook exits non-zero for operations matching guardrail patterns, blocking the tool call",
				"PostToolUse hook writes a JSONL event with tool name, arguments, and result summary",
				"Hook scripts are idempotent and safe to re-install",
				"Tests cover hook config generation, guardrail pattern matching, and event logging"
			],
			"validation_commands": ["uv run pytest tests/ -k hook -x --tb=short"],
			"passes": true
		},
		{
			"id": "AR-702",
			"priority": 1,
			"title": "Live Session Log Watcher",
			"description": "Add a file watcher (using watchdog or polling) that monitors Claude Code's JSONL session logs in real-time. When the developer runs Claude Code independently, Ralph can ingest tool calls, decisions, and output as they happen — feeding the Weather Model and updating the TUI activity console live.",
			"user_story": "As a developer running Claude Code in my normal terminal, I want Ralph to automatically observe what the AI is doing so that my GUARDRAILS and RECENT files stay fresh without me needing to manually sync.",
			"steps": [
				"Implement LogWatcher class that tails JSONL files for new events",
				"Support Claude Code's project log directory structure (~/.claude/projects/<hash>/)",
				"Parse incremental events and emit progress callbacks",
				"Add `ralph watch` CLI command to start the watcher",
				"Add TUI action to toggle the watcher",
				"Feed watched events into the existing iteration store"
			],
			"acceptance_criteria": [
				"LogWatcher detects new JSONL lines within 2 seconds of being written",
				"Events are parsed using the existing ClaudeCodeIngester patterns",
				"Watched events appear in the TUI activity console in real-time",
				"Watcher gracefully handles log rotation and file deletion",
				"Tests cover file watching, parsing, and edge cases"
			],
			"validation_commands": ["uv run pytest tests/ -k watch -x --tb=short"],
			"passes": true
		},
		{
			"id": "AR-703",
			"priority": 1,
			"title": "Embedded Terminal Panel",
			"description": "Add a split-pane terminal emulator to the TUI using Textual's Terminal widget backed by pyte. The developer can run their coding CLI inside the TUI while seeing the dashboard, activity log, and iteration status alongside it. Ralph can read the PTY output for automatic ingestion.",
			"user_story": "As a developer, I want to run Claude Code inside the Ralph TUI so that I can see my dashboard, guardrails, and forecast alongside the AI's work in a single terminal window.",
			"steps": [
				"Add textual-terminal dependency and Terminal widget",
				"Create split-pane layout: terminal left/bottom, dashboard right/top",
				"Wire PTY output to Ralph's ingest pipeline for passive learning",
				"Add palette action to toggle terminal panel",
				"Add keybinding to switch focus between terminal and dashboard",
				"Persist terminal panel visibility preference in config"
			],
			"acceptance_criteria": [
				"Terminal panel renders a full interactive shell inside the TUI",
				"Developer can launch Claude Code / Codex / OpenCode inside the panel",
				"Dashboard panels continue refreshing while terminal is active",
				"Terminal output is optionally forwarded to Ralph's event pipeline",
				"Tests cover panel toggle, focus switching, and layout persistence"
			],
			"validation_commands": ["uv run pytest tests/ -k terminal -x --tb=short"],
			"passes": true
		},
		{
			"id": "AR-704",
			"priority": 1,
			"title": "Iteration Diff Viewer",
			"description": "After each Ralph iteration, capture the git diff and display it in the TUI as a scrollable, syntax-highlighted panel. Developers can review what changed without leaving the TUI.",
			"user_story": "As a developer, I want to see exactly what Ralph's AI agent changed after each iteration so that I can quickly review and approve the changes.",
			"steps": [
				"Capture `git diff` output after each iteration in the Ralph loop",
				"Store diffs alongside iteration reports",
				"Add a DiffViewerModal to the TUI with syntax highlighting via Rich",
				"Add palette action 'View Last Diff'",
				"Support navigation through multiple iteration diffs"
			],
			"acceptance_criteria": [
				"After each iteration, the git diff is automatically captured and stored",
				"DiffViewerModal shows syntax-highlighted diff with file headers",
				"User can scroll through large diffs and navigate between files",
				"Tests cover diff capture, storage, and modal rendering"
			],
			"validation_commands": ["uv run pytest tests/ -k diff -x --tb=short"],
			"passes": true
		},
		{
			"id": "AR-705",
			"priority": 3,
			"title": "Token & Cost Tracking Dashboard",
			"description": "Track token usage and estimated API costs for each Ralph iteration. Display cumulative and per-item costs in the TUI dashboard. Support budget limits that pause the Ralph loop when exceeded.",
			"user_story": "As a developer, I want to know how much each PRD item costs in API tokens so that I can make informed decisions about when to let Ralph run autonomously vs. manually.",
			"steps": [
				"Parse token usage from Claude Code / Codex output or API response headers",
				"Add cost model with per-provider pricing tables",
				"Store per-iteration token counts in iteration reports",
				"Add cost summary panel to TUI dashboard",
				"Add budget limit config with auto-pause on exceeded",
				"Add `ralph cost-report` CLI command"
			],
			"acceptance_criteria": [
				"Token counts are captured from CLI output for each iteration",
				"Dashboard displays cumulative cost by PRD item",
				"Budget limit pauses the Ralph loop with a clear message",
				"Tests cover cost calculation, budget enforcement, and report generation"
			],
			"validation_commands": ["uv run pytest tests/ -k cost -x --tb=short"],
			"passes": false
		},
		{
			"id": "AR-706",
			"priority": 1,
			"title": "Desktop Notifications",
			"description": "Send OS-level notifications when key Ralph events occur: iteration complete, validation failure, loop finished, budget exceeded. Support macOS (osascript), Linux (notify-send), and Claude Code's Notification hook.",
			"user_story": "As a developer running Ralph in the background, I want desktop notifications so that I know immediately when something needs my attention.",
			"steps": [
				"Create platform-aware notification dispatcher",
				"Hook into Ralph loop progress callbacks for key events",
				"Add notification preferences to config (which events to notify)",
				"Wire Claude Code's Notification hook to Ralph notifications",
				"Add TUI toggle for notification preferences"
			],
			"acceptance_criteria": [
				"macOS notification fires on iteration complete/failure",
				"Notification preferences are persisted in config",
				"Claude Code Notification hook triggers Ralph notification pathway",
				"Tests cover notification dispatch with platform stubs"
			],
			"validation_commands": ["uv run pytest tests/ -k notif -x --tb=short"],
			"passes": true
		},
		{
			"id": "AR-707",
			"priority": 4,
			"title": "OpenCode Plugin Integration",
			"description": "Implement Ralph as an OpenCode plugin that subscribes to session lifecycle events (session start, idle, tool use). This enables the same hook-based workflow that Claude Code provides, but for the OpenCode ecosystem.",
			"user_story": "As a developer using OpenCode, I want Ralph to react to my coding sessions just like it does with Claude Code hooks.",
			"steps": [
				"Research OpenCode plugin API and subscription model",
				"Implement plugin that subscribes to session events",
				"Forward events to Ralph iteration store",
				"Add `ralph plugin opencode install` command",
				"Add TUI action for OpenCode plugin management"
			],
			"acceptance_criteria": [
				"OpenCode plugin receives session lifecycle events",
				"Events are forwarded to Ralph's iteration store",
				"Plugin installation and removal are idempotent",
				"Tests cover event subscription and forwarding"
			],
			"validation_commands": ["uv run pytest tests/ -k opencode_plugin -x --tb=short"],
			"passes": false
		},
		{
			"id": "AR-708",
			"priority": 1,
			"title": "Iteration Timeline View",
			"description": "Add a structured timeline panel to the TUI that visualizes iteration history with outcomes, durations, key decisions, and gotchas. Replace the plain-text progress.txt with an interactive, scrollable timeline.",
			"user_story": "As a developer, I want a visual timeline of Ralph's iterations so that I can quickly spot patterns, identify blockers, and understand the trajectory of my project.",
			"steps": [
				"Create TimelinePanel Textual widget rendering iteration reports",
				"Show outcome badges (✓/✗), duration, item ID, and summary",
				"Support drill-down into iteration details",
				"Add palette action to toggle timeline view",
				"Implement keyboard navigation through timeline entries"
			],
			"acceptance_criteria": [
				"Timeline renders all iteration reports with visual indicators",
				"Drill-down shows full iteration report including diff",
				"Timeline auto-scrolls to latest entry",
				"Tests cover timeline rendering and navigation"
			],
			"validation_commands": ["uv run pytest tests/ -k timeline -x --tb=short"],
			"passes": true
		},
		{
			"id": "AR-801",
			"priority": 1,
			"title": "Accumulative tier updates for GUARDRAILS and STYLE",
			"description": "Change Ralph synthesize-climate to merge new entries into existing GUARDRAILS.md and STYLE.md instead of overwriting. ClimateSynthesizer currently generates content from iteration reports only and replaces the entire tier file. It should read current tier content, extract new candidates from reports, use LLM to synthesize only the new items (excluding duplicates), and merge/append them to existing content using parse_tier_content/merge_tier_content.",
			"user_story": "As a developer running the Ralph loop, I want GUARDRAILS and STYLE to accumulate learnings over time so that each iteration adds to the knowledge base rather than replacing it.",
			"steps": [
				"Update ClimateSynthesizer.synthesize_guardrails to read current GUARDRAILS via files.read_tier",
				"Update synthesize_guardrails prompt to include current content and instruct LLM to return only NEW items not already present",
				"Merge synthesized bullets with existing using tier_format.parse_tier_content and merge_tier_content",
				"Update ClimateSynthesizer.synthesize_style with same read-merge-write flow",
				"Ensure deduplication against existing entries before adding",
				"Add tests for merge behavior and idempotent re-synthesis"
			],
			"acceptance_criteria": [
				"synthesize-climate reads existing GUARDRAILS.md and STYLE.md before generating",
				"New entries are appended/merged; existing entries are preserved",
				"Re-running synthesize-climate with no new iteration reports does not change tier content",
				"Round-trip through parse_tier_content and merge_tier_content preserves structure",
				"Tests cover merge, dedup, and no-op when no new candidates"
			],
			"validation_commands": ["uv run pytest tests/ -k synthesis -x --tb=short", "uv run ruff check .", "uv run ty check"],
			"passes": true
		},
		{
			"id": "AR-802",
			"priority": 1,
			"title": "Accumulative tier updates for RECENT",
			"description": "Change Ralph rebuild-forecast to merge new forecast content into existing RECENT.md instead of overwriting. ForecastGenerator.write_forecast currently replaces the entire file. It should append new iteration summaries to existing RECENT content (or merge forecast sections) so that history accumulates. When RECENT format includes both session summaries and forecast sections, preserve both and append new forecast output.",
			"user_story": "As a developer running the Ralph loop, I want RECENT.md to accumulate session summaries and forecast updates over time rather than being replaced each iteration.",
			"steps": [
				"Update ForecastGenerator.write_forecast to read current RECENT content",
				"Define merge strategy: append new forecast block or merge into existing sections",
				"Implement merge that preserves existing content and appends new forecast",
				"Handle empty RECENT (first run) by writing initial content",
				"Add tests for merge behavior"
			],
			"acceptance_criteria": [
				"rebuild-forecast reads existing RECENT.md before writing",
				"New forecast content is merged/appended; existing content is preserved",
				"First run with no RECENT creates initial content",
				"Tests cover merge and first-run behavior"
			],
			"validation_commands": ["uv run pytest tests/ -k forecast -x --tb=short", "uv run ruff check .", "uv run ty check"],
			"passes": true
		},
		{
			"id": "AR-803",
			"priority": 2,
			"title": "Token-based compaction trigger",
			"description": "Add configurable token threshold (default ~10000 tokens) for GUARDRAILS, STYLE, and RECENT. When any tier file exceeds the threshold, trigger compaction (distill/summarize) to reduce size while preserving the most important entries. Add config key e.g. compaction.max_tier_tokens with default 10000. Before or after tier writes, check estimated token count; if exceeded, run tier compaction or LLM summarization to bring size back under threshold.",
			"user_story": "As a developer with long-running Ralph sessions, I want tier files to compact automatically when they grow too large (~10k tokens) so that context stays manageable for the agent without losing critical learnings.",
			"steps": [
				"Add max_tier_tokens config (default 10000) to compaction or ralph config",
				"Implement token estimation (chars/4 or tiktoken if available) for tier content",
				"Add pre-write or post-write check: if tier would exceed threshold, run compaction first",
				"Wire compaction trigger into synthesize-climate and rebuild-forecast flows",
				"Add optional compact-before-write mode to avoid unbounded growth",
				"Add tests for threshold detection and compaction trigger"
			],
			"acceptance_criteria": [
				"Config max_tier_tokens (default 10000) controls when compaction runs",
				"When tier content exceeds threshold, compaction/summarization runs before or after write",
				"Compaction preserves highest-signal entries; reduces total token count",
				"Tests cover threshold check and compaction trigger logic"
			],
			"validation_commands": ["uv run pytest tests/ -k 'tier and (compact or token)' -x --tb=short", "uv run ruff check .", "uv run ty check"],
			"passes": false
		}
	]
}
